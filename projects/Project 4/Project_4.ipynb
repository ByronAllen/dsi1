{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import urllib2\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "import psycopg2\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame()\n",
    "pd.set_option('max_colwidth', 500)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def SQLFunc(x):\n",
    "\n",
    "    conn = psycopg2.connect(host = \"localhost\",\n",
    "                            database=\"allenbyron\",\n",
    "                            port=5432)\n",
    "\n",
    "    cur = conn.cursor()\n",
    "\n",
    "    #Create sql query and execute\n",
    "    sql_insert = 'insert into data_science_jobs (' + string.join(x[0], ', ') + ') values(' + ('%s, ' * (len(x) - 1)) + '%s' + ');'\n",
    "    cur.execute(sql_insert, [i for i in x[1]])\n",
    "\n",
    "    conn.commit()\n",
    "    cur.close()\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def DataScienceJobSearch(city):\n",
    "    \n",
    "    jobs_scrapped = 0 #not useful when salary bins are taken into account \n",
    "\n",
    "    for c in city:        \n",
    "        if c == 'Sydney' or c == 'Melbourne':\n",
    "            home_url = 'http://au.indeed.com'\n",
    "            base_url = 'http://au.indeed.com/jobs?q=data+scientist'\n",
    "            loc_url = base_url + '&l=' + c\n",
    "        else:\n",
    "            home_url = 'http://www.indeed.com'\n",
    "            base_url = 'http://www.indeed.com/jobs?q=data+scientist'\n",
    "            loc_url = base_url + '&l=' + c\n",
    "        \n",
    "        #Identify salary bins\n",
    "        estimated_salary = BeautifulSoup(urllib2.urlopen(loc_url), \"lxml\")\n",
    "        rbList = estimated_salary.find('ul', attrs={'class': 'rbList'})\n",
    "        salary_tuple = zip([i.getText() for i in rbList.find_all('a')],[i.get('href') for i in rbList.find_all('a')]) \n",
    "        print loc_url\n",
    "        \n",
    "        for s in salary_tuple:\n",
    "            \n",
    "            #Define salary bins\n",
    "            salary_bins = s[0]\n",
    "            bin_as_url_elem = salary_bins.split(',')[0] \n",
    "            \n",
    "            salary_url = home_url + s[1]\n",
    "            print salary_url\n",
    "            content_for_range = BeautifulSoup(urllib2.urlopen(salary_url), \"lxml\")\n",
    "\n",
    "            #Define max of range\n",
    "            try:\n",
    "                searchCount = content_for_range.find('div', attrs={'id': 'searchCount'}).getText().strip()\n",
    "                number_of_results = str(searchCount).split(' ')\n",
    "                max_of_range = int(number_of_results[-1]) \n",
    "                var = int(round((max_of_range/10)) + 2)\n",
    "            except:\n",
    "                var = 100\n",
    "            \n",
    "            #change max of range to var if not var\n",
    "            for page in range(1, var): \n",
    "                page = (page-1) * 10  \n",
    "                start_of_url = '/jobs?q=data+scientist+%24'\n",
    "                mid_url = '%2C000&l='\n",
    "                start_from = '&start='  # start page number\n",
    "                url = \"%s%s%s%s%s%s%d\" % (home_url, start_of_url, bin_as_url_elem, mid_url, c, start_from, page) \n",
    "                print url\n",
    "                \n",
    "                contents = BeautifulSoup(urllib2.urlopen(url), \"lxml\") \n",
    "                contentsElements = contents.find_all('div', attrs={'class': ' row result'}) \n",
    "\n",
    "                for job in contentsElements:\n",
    "                    \n",
    "                    jobs_scrapped += 1         \n",
    "                    \n",
    "                    #Details of the job\n",
    "                    try: job_title = job.find('h2', attrs={'class':'jobtitle'}).getText().strip()\n",
    "                    except: job_title = '-' #None\n",
    "\n",
    "                    try: company = job.find('span', attrs={'itemprop':'name'}).getText().strip()\n",
    "                    except: company = '-' #None\n",
    "\n",
    "                    try: salary = job.find('nobr').getText().strip() \n",
    "                    except: salary = '-' #None\n",
    "\n",
    "                    try: reviews_total = job.find('span', attrs={'class':'slNoUnderline'}).getText().strip()\n",
    "                    except: reviews_total = '-' #None\n",
    "\n",
    "                    try: job_description = job.find('span', attrs={'itemprop':'description'}).getText()\n",
    "                    except: job_description = '-' #None\n",
    "\n",
    "                    try: link = \"%s%s\" % (home_url, job.find('a').get('href'))\n",
    "                    except: link = '-' #None\n",
    "\n",
    "                    try: job_addr = job.find('span', attrs={'itemprop':'addressLocality'}).getText()\n",
    "                    except: job_addr = '-' #None\n",
    "\n",
    "                    try: job_posted = job.find('span', attrs={'class': 'date'}).getText()   \n",
    "                    except: job_posted = '-' #None\n",
    "\n",
    "\n",
    "\n",
    "                    #Company page\n",
    "                    try:\n",
    "                        indeed_company_url = \"%s%s%s\" % (home_url, '/cmp/', company.replace(' ','-'))\n",
    "                        indeed_company_page = BeautifulSoup(urllib2.urlopen(indeed_company_url), \"lxml\")\n",
    "                    except:\n",
    "                        #print Exception, 'problem with indeed_company_url', company\n",
    "                        continue\n",
    "\n",
    "\n",
    "                    #Glean overall company rating \n",
    "                    try:\n",
    "                        company_overall_ratingElements = indeed_company_page.find_all('div', attrs={'id':'cmp-reviews'})\n",
    "                        for score in company_overall_ratingElements:\n",
    "                            reviews_score = score.find('span', attrs={'class':'cmp-average-rating'}).getText().strip()\n",
    "                    except:\n",
    "                        reviews_score = '-' #None\n",
    "\n",
    "\n",
    "                    #Glean various company ratings\n",
    "                    try:\n",
    "                        specific_ratingsElements = indeed_company_page.find('dl', attrs={'id':'cmp-reviews-attributes'})    \n",
    "                        specific_ratingsElements = specific_ratingsElements.find_all('span', attrs={'class': 'cmp-star-rating'})\n",
    "\n",
    "                        if specific_ratingsElements != None:\n",
    "\n",
    "                            #print 'specific_ratingsElements'\n",
    "\n",
    "                            if specific_ratingsElements[0] != None:\n",
    "                                #print 'first element has something'\n",
    "\n",
    "                                try: work_life_bal = specific_ratingsElements[0].getText()\n",
    "                                except: work_life_bal = '-' #None\n",
    "\n",
    "                                try: salary_benefits = specific_ratingsElements[1].getText()\n",
    "                                except: salary_benefits = '-' #None\n",
    "\n",
    "                                try: job_security_advance = specific_ratingsElements[2].getText()\n",
    "                                except: job_security_advance = '-' #None\n",
    "\n",
    "                                try: management = specific_ratingsElements[3].getText()\n",
    "                                except: management = '-' #None\n",
    "\n",
    "                                try: culture = specific_ratingsElements[4].getText()\n",
    "                                except: culture = '-' #None        \n",
    "\n",
    "                            else:\n",
    "                                #print 'first element has nothing'\n",
    "\n",
    "                                try: work_life_bal = specific_ratingsElements[1].getText()\n",
    "                                except: work_life_bal = '-' #None\n",
    "\n",
    "                                try: salary_benefits = specific_ratingsElements[2].getText()\n",
    "                                except: salary_benefits = '-' #None\n",
    "\n",
    "                                try: job_security_advance = specific_ratingsElements[3].getText()\n",
    "                                except: job_security_advance = '-' #None\n",
    "\n",
    "                                try: management = specific_ratingsElements[4].getText()\n",
    "                                except: management = '-' #None\n",
    "\n",
    "                                try: culture = specific_ratingsElements[5].getText()\n",
    "                                except: culture = '-' #None   \n",
    "\n",
    "                    except:\n",
    "                        #print 'specific_ratingsElements is empty'\n",
    "\n",
    "                        work_life_bal = '-' #None\n",
    "                        salary_benefits = '-' #None\n",
    "                        job_security_advance = '-' #None\n",
    "                        management = '-' #None\n",
    "                        culture = '-' #None   \n",
    "\n",
    "                    #print '...........................'\n",
    "\n",
    "                    #df = df.append({fields, ignore_index=True)\n",
    "                    fields = {'job_title': job_title,\n",
    "                                'company': company,\n",
    "                                'salary': salary,\n",
    "                                'reviews_total': reviews_total,\n",
    "                                'reviews_score': reviews_score,\n",
    "                                'job_description': job_description,\n",
    "                                'link': link,\n",
    "                                'job_addr': job_addr,\n",
    "                                'job_posted': job_posted,\n",
    "                                'work_life_bal': work_life_bal,\n",
    "                                'salary_benefits': salary_benefits,\n",
    "                                'job_security_advance': job_security_advance,\n",
    "                                'management': management,\n",
    "                                'culture': culture,\n",
    "                                'salary_bins': salary_bins\n",
    "                              }\n",
    "                    \n",
    "                    SQLFunc(fields)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find_all'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-8ef5fd7f9580>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mlist_of_cities\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'Sydney'\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0;34m'Melbourne'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'San+Francisco'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mjob_searches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataScienceJobSearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist_of_cities\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mjob_searches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-8800953853cd>\u001b[0m in \u001b[0;36mDataScienceJobSearch\u001b[0;34m(city)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mestimated_salary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murllib2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"lxml\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mrbList\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mestimated_salary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ul'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'class'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'rbList'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0msalary_tuple\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetText\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrbList\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'a'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'href'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrbList\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'a'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0;32mprint\u001b[0m \u001b[0mloc_url\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find_all'"
     ]
    }
   ],
   "source": [
    "list_of_cities = ['Sydney' ,'Melbourne', 'San+Francisco']\n",
    "job_searches = DataScienceJobSearch(list_of_cities)\n",
    "job_searches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1326, 16)\n"
     ]
    }
   ],
   "source": [
    "print job_searches.shape\n",
    "job_searches.drop_duplicates(subset = ['company', 'job_description', 'job_title', 'link'], keep='last',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "job_searches.reviews_score = job_searches.reviews_score.astype(np.float_)\n",
    "job_searches['salary_bins_int'] = [x.replace('$', '').replace(',', '').replace('+','') for x in job_searches.salary_bins]\n",
    "job_searches.reviews_total = [x.replace(' reviews', '') for x in job_searches.reviews_total]\n",
    "job_searches.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "reload(sys)\n",
    "sys.setdefaultencoding('utf-8')\n",
    "job_searches.to_csv('job_searches.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
