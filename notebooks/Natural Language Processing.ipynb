{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Data Science with Python \n",
    "## General Assembly\n",
    "## Natural Language Processing (NLP)\n",
    "\n",
    "Make sure you have installed nltk and downloaded the following copora:\n",
    "\n",
    "* punkt\n",
    "* gutenberg\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Part 1\n",
    "\n",
    "#### Tokenization\n",
    "\n",
    "What:  Separate text into units such as sentences or words\n",
    "\n",
    "Why:   Gives structure to previously unstructured text\n",
    "\n",
    "Notes: Relatively easy with English language text, not easy with some languages\n",
    "\n",
    "\n",
    "\"corpus\" = collection of documents\n",
    "\n",
    "\"corpora\" = plural form of corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "import sklearn.linear_model\n",
    "import sklearn.cross_validation\n",
    "import sklearn.metrics\n",
    "import nltk\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'austen-emma.txt',\n",
       " u'austen-persuasion.txt',\n",
       " u'austen-sense.txt',\n",
       " u'bible-kjv.txt',\n",
       " u'blake-poems.txt',\n",
       " u'bryant-stories.txt',\n",
       " u'burgess-busterbrown.txt',\n",
       " u'carroll-alice.txt',\n",
       " u'chesterton-ball.txt',\n",
       " u'chesterton-brown.txt',\n",
       " u'chesterton-thursday.txt',\n",
       " u'edgeworth-parents.txt',\n",
       " u'melville-moby_dick.txt',\n",
       " u'milton-paradise.txt',\n",
       " u'shakespeare-caesar.txt',\n",
       " u'shakespeare-hamlet.txt',\n",
       " u'shakespeare-macbeth.txt',\n",
       " u'whitman-leaves.txt']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the NLTK library, and use ntlk.corpus.gutenberg.fileids() to\n",
    "# find the filenames for Jane Austen's Emma and Lewis Carrol's Alice in \n",
    "# Wonderland\n",
    "\n",
    "nltk.corpus.gutenberg.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Break these novels up into sentences. Put these sentence lists into\n",
    "# a list so that you can use it later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#split austen-emma.txt\n",
    "#split carroll-alice.txt\n",
    "\n",
    "emma_aslist = nltk.sent_tokenize(nltk.corpus.gutenberg.raw('austen-emma.txt'))\n",
    "alice_aslist = nltk.sent_tokenize(nltk.corpus.gutenberg.raw('carroll-alice.txt'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences in Emma: 7493\n",
      "Number of sentences in Alice: 1625\n"
     ]
    }
   ],
   "source": [
    "# Count the number of sentences in each novel.\n",
    "\n",
    "print 'Number of sentences in Emma:', len(emma_aslist)\n",
    "print 'Number of sentences in Alice:', len(alice_aslist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Break each sentence up into words. You will end up with a \n",
    "# list of lists of words for Emma and another one for Alice in\n",
    "# Wonderland\n",
    "\n",
    "emma_list_of_words_per_sent = []\n",
    "alice_list_of_words_per_sent = []  \n",
    "\n",
    "for sent in emma_aslist:\n",
    "    words = [x for x in nltk.word_tokenize(sent) if x not in ['.','?']]\n",
    "    emma_list_of_words_per_sent.append(words)\n",
    "    \n",
    "for sent2 in alice_aslist:\n",
    "    words2 = [x2 for x2 in nltk.word_tokenize(sent2) if x2 not in ['.','?']]\n",
    "    alice_list_of_words_per_sent.append(words2)\n",
    "    \n",
    "alice_list_of_words_per_sent == emma_list_of_words_per_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg. words per sentence in Emma:  24\n",
      "Avg. words per sentence in Alice:  19\n"
     ]
    }
   ],
   "source": [
    "# Count the number of words in each sentence\n",
    "\n",
    "emma_wc_per_sent = [len(x) for x in emma_list_of_words_per_sent]\n",
    "alice_wc_per_sent = [len(x) for x in alice_list_of_words_per_sent]\n",
    "\n",
    "print 'Avg. words per sentence in Emma: ', sum(emma_wc_per_sent) / len(emma_aslist)\n",
    "print 'Avg. words per sentence in Alice: ', sum(alice_wc_per_sent) / len(alice_aslist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Which novel has more average words per sentence?\n",
    "\n",
    "#Emma!\n",
    "\n",
    "# Given their target audience, is this what you would expect?\n",
    "\n",
    "#Yes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a flat list (i.e. not a list of lists) of words in\n",
    "# the two novels\n",
    "\n",
    "emma_words_used = set()\n",
    "alice_words_used = set()\n",
    "    \n",
    "for i in emma_list_of_words_per_sent:\n",
    "    for t in i:\n",
    "        emma_words_used.add(t)\n",
    "\n",
    "for a in alice_list_of_words_per_sent:\n",
    "    for l in a:\n",
    "        alice_words_used.add(l)\n",
    "        \n",
    "emma_words_used == alice_words_used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For each novel, construct a set of all the distinct words used\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lexical Div. for Emma:  0.0458264075757\n",
      "Lexical Div. for Alice:  0.0991815267793\n"
     ]
    }
   ],
   "source": [
    "# Calculate the lexical diversity of each novel (distinct words / word count)\n",
    "\n",
    "emma_lexdiv = float(len(emma_words_used)) / float(sum(emma_wc_per_sent))\n",
    "alice_lexdiv = float(len(alice_words_used)) / float(sum(alice_wc_per_sent))\n",
    "\n",
    "print 'Lexical Div. for Emma: ', emma_lexdiv\n",
    "print 'Lexical Div. for Alice: ', alice_lexdiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'Emma'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emma_list_of_words_per_sent[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n",
      "working...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>book_name</th>\n",
       "      <th>year</th>\n",
       "      <th>lexical_diversity</th>\n",
       "      <th>avg_sentence_length</th>\n",
       "      <th>target_audience</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>austen-emma.txt</td>\n",
       "      <td>Emma</td>\n",
       "      <td>[Emma by Jane Austen 1816]\\n\\nVOLUME I\\n\\nCHAP...</td>\n",
       "      <td>0.045826</td>\n",
       "      <td>24.649273</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>austen-persuasion.txt</td>\n",
       "      <td>Persuasion</td>\n",
       "      <td>[Persuasion by Jane Austen 1818]\\n\\n\\nChapter ...</td>\n",
       "      <td>0.066186</td>\n",
       "      <td>25.876300</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>austen-sense.txt</td>\n",
       "      <td>Sense</td>\n",
       "      <td>[Sense and Sensibility by Jane Austen 1811]\\n\\...</td>\n",
       "      <td>0.051939</td>\n",
       "      <td>28.324436</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bible-kjv.txt</td>\n",
       "      <td>The</td>\n",
       "      <td>[The King James Bible]\\n\\nThe Old Testament of...</td>\n",
       "      <td>0.019830</td>\n",
       "      <td>30.769925</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>blake-poems.txt</td>\n",
       "      <td>Poems</td>\n",
       "      <td>[Poems by William Blake 1789]\\n\\n \\nSONGS OF I...</td>\n",
       "      <td>0.227964</td>\n",
       "      <td>22.402817</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>bryant-stories.txt</td>\n",
       "      <td>Stories</td>\n",
       "      <td>[Stories to Tell to Children by Sara Cone Brya...</td>\n",
       "      <td>0.083938</td>\n",
       "      <td>19.658564</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>burgess-busterbrown.txt</td>\n",
       "      <td>The</td>\n",
       "      <td>[The Adventures of Buster Bear by Thornton W. ...</td>\n",
       "      <td>0.099253</td>\n",
       "      <td>17.644356</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>carroll-alice.txt</td>\n",
       "      <td>Alice</td>\n",
       "      <td>[Alice's Adventures in Wonderland by Lewis Car...</td>\n",
       "      <td>0.099182</td>\n",
       "      <td>19.774154</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>chesterton-ball.txt</td>\n",
       "      <td>The</td>\n",
       "      <td>[The Ball and The Cross by G.K. Chesterton 190...</td>\n",
       "      <td>0.097878</td>\n",
       "      <td>20.068988</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>chesterton-brown.txt</td>\n",
       "      <td>The</td>\n",
       "      <td>[The Wisdom of Father Brown by G. K. Chesterto...</td>\n",
       "      <td>0.104439</td>\n",
       "      <td>22.041487</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>chesterton-thursday.txt</td>\n",
       "      <td>The</td>\n",
       "      <td>[The Man Who Was Thursday by G. K. Chesterton ...</td>\n",
       "      <td>0.105066</td>\n",
       "      <td>18.412207</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>edgeworth-parents.txt</td>\n",
       "      <td>The</td>\n",
       "      <td>[The Parent's Assistant, by Maria Edgeworth]\\r...</td>\n",
       "      <td>0.049685</td>\n",
       "      <td>19.835578</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>melville-moby_dick.txt</td>\n",
       "      <td>Moby</td>\n",
       "      <td>[Moby Dick by Herman Melville 1851]\\r\\n\\r\\n\\r\\...</td>\n",
       "      <td>0.084130</td>\n",
       "      <td>25.038266</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>milton-paradise.txt</td>\n",
       "      <td>Paradise</td>\n",
       "      <td>[Paradise Lost by John Milton 1667] \\n \\n \\nBo...</td>\n",
       "      <td>0.116733</td>\n",
       "      <td>51.287193</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>shakespeare-caesar.txt</td>\n",
       "      <td>The</td>\n",
       "      <td>[The Tragedie of Julius Caesar by William Shak...</td>\n",
       "      <td>0.152669</td>\n",
       "      <td>14.853015</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>shakespeare-hamlet.txt</td>\n",
       "      <td>The</td>\n",
       "      <td>[The Tragedie of Hamlet by William Shakespeare...</td>\n",
       "      <td>0.162940</td>\n",
       "      <td>14.432272</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>shakespeare-macbeth.txt</td>\n",
       "      <td>The</td>\n",
       "      <td>[The Tragedie of Macbeth by William Shakespear...</td>\n",
       "      <td>0.197195</td>\n",
       "      <td>14.164505</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>whitman-leaves.txt</td>\n",
       "      <td>Leaves</td>\n",
       "      <td>[Leaves of Grass by Walt Whitman 1855]\\n\\n\\nCo...</td>\n",
       "      <td>0.106083</td>\n",
       "      <td>38.376274</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  file_name   book_name  \\\n",
       "0           austen-emma.txt        Emma   \n",
       "1     austen-persuasion.txt  Persuasion   \n",
       "2          austen-sense.txt       Sense   \n",
       "3             bible-kjv.txt         The   \n",
       "4           blake-poems.txt       Poems   \n",
       "5        bryant-stories.txt     Stories   \n",
       "6   burgess-busterbrown.txt         The   \n",
       "7         carroll-alice.txt       Alice   \n",
       "8       chesterton-ball.txt         The   \n",
       "9      chesterton-brown.txt         The   \n",
       "10  chesterton-thursday.txt         The   \n",
       "11    edgeworth-parents.txt         The   \n",
       "12   melville-moby_dick.txt        Moby   \n",
       "13      milton-paradise.txt    Paradise   \n",
       "14   shakespeare-caesar.txt         The   \n",
       "15   shakespeare-hamlet.txt         The   \n",
       "16  shakespeare-macbeth.txt         The   \n",
       "17       whitman-leaves.txt      Leaves   \n",
       "\n",
       "                                                 year  lexical_diversity  \\\n",
       "0   [Emma by Jane Austen 1816]\\n\\nVOLUME I\\n\\nCHAP...           0.045826   \n",
       "1   [Persuasion by Jane Austen 1818]\\n\\n\\nChapter ...           0.066186   \n",
       "2   [Sense and Sensibility by Jane Austen 1811]\\n\\...           0.051939   \n",
       "3   [The King James Bible]\\n\\nThe Old Testament of...           0.019830   \n",
       "4   [Poems by William Blake 1789]\\n\\n \\nSONGS OF I...           0.227964   \n",
       "5   [Stories to Tell to Children by Sara Cone Brya...           0.083938   \n",
       "6   [The Adventures of Buster Bear by Thornton W. ...           0.099253   \n",
       "7   [Alice's Adventures in Wonderland by Lewis Car...           0.099182   \n",
       "8   [The Ball and The Cross by G.K. Chesterton 190...           0.097878   \n",
       "9   [The Wisdom of Father Brown by G. K. Chesterto...           0.104439   \n",
       "10  [The Man Who Was Thursday by G. K. Chesterton ...           0.105066   \n",
       "11  [The Parent's Assistant, by Maria Edgeworth]\\r...           0.049685   \n",
       "12  [Moby Dick by Herman Melville 1851]\\r\\n\\r\\n\\r\\...           0.084130   \n",
       "13  [Paradise Lost by John Milton 1667] \\n \\n \\nBo...           0.116733   \n",
       "14  [The Tragedie of Julius Caesar by William Shak...           0.152669   \n",
       "15  [The Tragedie of Hamlet by William Shakespeare...           0.162940   \n",
       "16  [The Tragedie of Macbeth by William Shakespear...           0.197195   \n",
       "17  [Leaves of Grass by Walt Whitman 1855]\\n\\n\\nCo...           0.106083   \n",
       "\n",
       "    avg_sentence_length target_audience  \n",
       "0             24.649273             NaN  \n",
       "1             25.876300             NaN  \n",
       "2             28.324436             NaN  \n",
       "3             30.769925             NaN  \n",
       "4             22.402817             NaN  \n",
       "5             19.658564             NaN  \n",
       "6             17.644356             NaN  \n",
       "7             19.774154             NaN  \n",
       "8             20.068988             NaN  \n",
       "9             22.041487             NaN  \n",
       "10            18.412207             NaN  \n",
       "11            19.835578             NaN  \n",
       "12            25.038266             NaN  \n",
       "13            51.287193             NaN  \n",
       "14            14.853015             NaN  \n",
       "15            14.432272             NaN  \n",
       "16            14.164505             NaN  \n",
       "17            38.376274             NaN  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (Optional, only for the very keen)\n",
    "# Repeat the above analysis for all the Gutenberg samples\n",
    "# Create a dataframe with the \n",
    "\n",
    "#=======================\n",
    "\n",
    "column_names = ['file_name',\n",
    "                'book_name', \n",
    "                'year',\n",
    "                'lexical_diversity', \n",
    "                'avg_sentence_length',\n",
    "                'target_audience']\n",
    "\n",
    "#target_audience_children # Can you use logistic regression to predict the audience, based on the content?\n",
    "\n",
    "#=========================\n",
    "\n",
    "#Part 0\n",
    "\n",
    "df = pd.DataFrame(columns=column_names)\n",
    "\n",
    "list_of_gut_books = ['austen-emma.txt',\n",
    "                    'austen-persuasion.txt',\n",
    "                    'austen-sense.txt',\n",
    "                    'bible-kjv.txt',\n",
    "                    'blake-poems.txt',\n",
    "                    'bryant-stories.txt',\n",
    "                    'burgess-busterbrown.txt',\n",
    "                    'carroll-alice.txt',\n",
    "                    'chesterton-ball.txt',\n",
    "                    'chesterton-brown.txt',\n",
    "                    'chesterton-thursday.txt',\n",
    "                    'edgeworth-parents.txt',\n",
    "                    'melville-moby_dick.txt',\n",
    "                    'milton-paradise.txt',\n",
    "                    'shakespeare-caesar.txt',\n",
    "                    'shakespeare-hamlet.txt',\n",
    "                    'shakespeare-macbeth.txt',\n",
    "                    'whitman-leaves.txt']\n",
    "\n",
    "book_name_list = []                       \n",
    "year_list = []                            \n",
    "lexical_diversity_list = []          \n",
    "avg_sentence_length_list = []        \n",
    "#target_audience_list = []\n",
    "\n",
    "for book in list_of_gut_books:\n",
    "        \n",
    "    #Part 1\n",
    "    book_aslist = nltk.sent_tokenize(nltk.corpus.gutenberg.raw(book))\n",
    "    year_list.append(book_aslist[0])\n",
    "\n",
    "    #Part 2\n",
    "    book_list_of_words_per_sent = []\n",
    "    for sent in book_aslist:\n",
    "        words = [x for x in nltk.word_tokenize(sent) if x not in ['.','?']]\n",
    "        book_list_of_words_per_sent.append(words)\n",
    "    \n",
    "    book_name_list.append(book_list_of_words_per_sent[0][1])\n",
    "        \n",
    "    #Part 3\n",
    "    book_wc_per_sent = [len(x) for x in book_list_of_words_per_sent]\n",
    "    avg_sentence_length = float(sum(book_wc_per_sent)) / float(len(book_aslist))\n",
    "    avg_sentence_length_list.append(avg_sentence_length)\n",
    "    \n",
    "    #Part 4\n",
    "    book_words_used = set()\n",
    "    for i in book_list_of_words_per_sent:\n",
    "        for t in i:\n",
    "            book_words_used.add(t)\n",
    "\n",
    "    #Part 5\n",
    "    book_lexdiv = float(len(book_words_used)) / float(sum(book_wc_per_sent))\n",
    "    lexical_diversity_list.append(book_lexdiv)\n",
    "\n",
    "    print 'working...'\n",
    "    \n",
    "#Part 6\n",
    "df.file_name = list_of_gut_books\n",
    "df.book_name = book_name_list\n",
    "df.year = year_list\n",
    "df.lexical_diversity = lexical_diversity_list\n",
    "df.avg_sentence_length = avg_sentence_length_list\n",
    "#df.target_audience = target_audience_list\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>book_name</th>\n",
       "      <th>year</th>\n",
       "      <th>lexical_diversity</th>\n",
       "      <th>avg_sentence_length</th>\n",
       "      <th>target_audience</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>austen-emma.txt</td>\n",
       "      <td>Emma</td>\n",
       "      <td>[Emma by Jane Austen 1816]\\n\\nVOLUME I\\n\\nCHAP...</td>\n",
       "      <td>0.045826</td>\n",
       "      <td>24.649273</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>austen-persuasion.txt</td>\n",
       "      <td>Persuasion</td>\n",
       "      <td>[Persuasion by Jane Austen 1818]\\n\\n\\nChapter ...</td>\n",
       "      <td>0.066186</td>\n",
       "      <td>25.876300</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>austen-sense.txt</td>\n",
       "      <td>Sense</td>\n",
       "      <td>[Sense and Sensibility by Jane Austen 1811]\\n\\...</td>\n",
       "      <td>0.051939</td>\n",
       "      <td>28.324436</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bible-kjv.txt</td>\n",
       "      <td>The</td>\n",
       "      <td>[The King James Bible]\\n\\nThe Old Testament of...</td>\n",
       "      <td>0.019830</td>\n",
       "      <td>30.769925</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>blake-poems.txt</td>\n",
       "      <td>Poems</td>\n",
       "      <td>[Poems by William Blake 1789]\\n\\n \\nSONGS OF I...</td>\n",
       "      <td>0.227964</td>\n",
       "      <td>22.402817</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               file_name   book_name  \\\n",
       "0        austen-emma.txt        Emma   \n",
       "1  austen-persuasion.txt  Persuasion   \n",
       "2       austen-sense.txt       Sense   \n",
       "3          bible-kjv.txt         The   \n",
       "4        blake-poems.txt       Poems   \n",
       "\n",
       "                                                year  lexical_diversity  \\\n",
       "0  [Emma by Jane Austen 1816]\\n\\nVOLUME I\\n\\nCHAP...           0.045826   \n",
       "1  [Persuasion by Jane Austen 1818]\\n\\n\\nChapter ...           0.066186   \n",
       "2  [Sense and Sensibility by Jane Austen 1811]\\n\\...           0.051939   \n",
       "3  [The King James Bible]\\n\\nThe Old Testament of...           0.019830   \n",
       "4  [Poems by William Blake 1789]\\n\\n \\nSONGS OF I...           0.227964   \n",
       "\n",
       "   avg_sentence_length target_audience  \n",
       "0            24.649273             NaN  \n",
       "1            25.876300             NaN  \n",
       "2            28.324436             NaN  \n",
       "3            30.769925             NaN  \n",
       "4            22.402817             NaN  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lexical_diversity</th>\n",
       "      <th>avg_sentence_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>lexical_diversity</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.274067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avg_sentence_length</th>\n",
       "      <td>-0.274067</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     lexical_diversity  avg_sentence_length\n",
       "lexical_diversity             1.000000            -0.274067\n",
       "avg_sentence_length          -0.274067             1.000000"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = df[['avg_sentence_length']]\n",
    "y = df.lexical_diversity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x11efd1b10>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAECCAYAAAAciLtvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x105bd0f50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAECCAYAAAASDQdFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x106f49cd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method LinearRegression.score of LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)>\n",
      "[-0.00757562]\n",
      "0.260779546538\n"
     ]
    }
   ],
   "source": [
    "ransac = sklearn.linear_model.RANSACRegressor()\n",
    "ransac.fit(x,y)\n",
    "prediction = ransac.predict(x)\n",
    "\n",
    "plt.scatter(x,y)\n",
    "plt.plot(x, prediction, 'r--')\n",
    "plt.show()\n",
    "\n",
    "print ransac.estimator_.score\n",
    "print ransac.estimator_.coef_\n",
    "print ransac.estimator_.intercept_\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Part 2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make nltk.Text objects from the two novels\n",
    "\n",
    "words = nltk.word_tokenize(nltk.corpus.gutenberg.raw('austen-emma.txt'))\n",
    "emma = nltk.Text(words)\n",
    "\n",
    "words = nltk.word_tokenize(nltk.corpus.gutenberg.raw('carroll-alice.txt'))\n",
    "alice = nltk.Text(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 5 of 5 matches:\n",
      " 'Hold your tongue , Ma ! ' said the young Crab , a little snappishly . 'You 'r\n",
      "You are old , Father William , ' the young man said , 'And your hair has become\n",
      "m getting tired of this . I vote the young lady tells us a story . ' 'I 'm afra\n",
      " ! ' said the Queen , 'and take this young lady to see the Mock Turtle , and to\n",
      "ears , but said nothing . 'This here young lady , ' said the Gryphon , 'she wan\n"
     ]
    }
   ],
   "source": [
    "# Does Jane Austen ever mention the word 'young' in Emma? What about Lewis Carroll?\n",
    "\n",
    "#emma.concordance('young')\n",
    "alice.concordance('young')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accomplished_woman worthy_man the_farmer the_man unexceptionable_man\n",
      "a_man so_as amiable_man pretty_woman the_are a_man's too_: pert_lawyer\n",
      "of_person alarming_man ,_cox a_woman too_; the_woman of_men\n"
     ]
    }
   ],
   "source": [
    "# What are the common contexts for these words?\n",
    "emma.common_contexts(['young'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX8AAAEZCAYAAAB/6SUgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nS1JBxl+SCjL+klSQ8Zekgoy/JBX0/+SjKENTbnNGAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1267797d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Where does the word 'cat' appear in Alice and Wonderland?\n",
    "alice.dispersion_plot(['cat'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Part 3\n",
    "\n",
    "###Stemming\n",
    "What:  Reduce a word to its base/stem form\n",
    "\n",
    "Why:   Often makes sense to treat multiple word forms the same way\n",
    "\n",
    "Notes: Uses a \"simple\" and fast rule-based approach\n",
    "       Output can be undesirable for irregular words\n",
    "       Stemmed words are usually not shown to users (used for analysis/indexing)\n",
    "       Some search engines treat words with the same stem as synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk.stem.snowball \n",
    "import nltk.stem.porter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create an English stemmer that uses the Snowball technique\n",
    "\n",
    "snowball = nltk.stem.snowball.SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "charg\n",
      "charg\n",
      "charg\n"
     ]
    }
   ],
   "source": [
    "# Stem the following words: charge, charging, charged\n",
    "print snowball.stem('charge')\n",
    "print snowball.stem('charging')\n",
    "print snowball.stem('charged')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ac&dc\n",
      "ac.dc\n",
      "akb48\n",
      "a48bk\n"
     ]
    }
   ],
   "source": [
    "# Can you stem \"words\" with punctuation in them? Or which have no letters?\n",
    "\n",
    "print snowball.stem('AC&DC')\n",
    "print snowball.stem('AC.DC')\n",
    "print snowball.stem('AKB48')\n",
    "print snowball.stem('A48BK')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all words:  8464\n",
      "all words:  8194\n"
     ]
    }
   ],
   "source": [
    "# Create a new list of words from the novels by dropping out spurious non-words.\n",
    "# You might find word_is_just_letters() helpful\n",
    "import re\n",
    "\n",
    "emma_words_used_refined = set()\n",
    "\n",
    "def word_is_just_letters(x):\n",
    "    for word in x:\n",
    "        keep = re.search('^[a-zA-Z]+', word)\n",
    "        if keep is not None:\n",
    "            emma_words_used_refined.add(word) \n",
    "\n",
    "word_is_just_letters(emma_words_used)\n",
    "\n",
    "print 'all words: ', len(emma_words_used)\n",
    "print 'all words: ', len(emma_words_used_refined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4994\n"
     ]
    }
   ],
   "source": [
    "# Stem all those words\n",
    "\n",
    "emma_stemmed_words = set()\n",
    "\n",
    "for w in emma_words_used_refined:\n",
    "    stem = snowball.stem(w)\n",
    "    emma_stemmed_words.add(stem)\n",
    "    \n",
    "print len(emma_stemmed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create two collections.Counter objects (one for each novel)\n",
    "# so that you can easily count word stems. If you give\n",
    "# the stemmed lists as an argument to constructor, \n",
    "# you can use .most_common(25) to get the top 25 tokens\n",
    "\n",
    "from collections import Counter\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'inexperi', 1),\n",
       " (u'nurseri', 1),\n",
       " (u'overcome.', 1),\n",
       " (u'pardon', 1),\n",
       " (u'knight-errantri', 1),\n",
       " (u'resist', 1),\n",
       " (u'better.', 1),\n",
       " (u'over-salt', 1),\n",
       " (u'yellow', 1),\n",
       " (u'interchang', 1)]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#using the most recent group of stemmed words; problem in that all are unique\n",
    "Counter(emma_stemmed_words).most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'the', 5198),\n",
       " (u'to', 5179),\n",
       " (u'and', 4875),\n",
       " (u'of', 4284),\n",
       " (u'i', 3164),\n",
       " (u'a', 3124),\n",
       " (u'it', 2622),\n",
       " (u'her', 2468),\n",
       " (u'was', 2396),\n",
       " (u'she', 2336)]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emma_words_used_refined = []\n",
    "emma_stemmed_words2 = []\n",
    "\n",
    "def word_is_just_letters(x):\n",
    "    for word in x:\n",
    "        keep = re.search('^[a-zA-Z]+', word)\n",
    "        if keep is not None:\n",
    "            emma_words_used_refined.append(word) \n",
    "\n",
    "word_is_just_letters(emma)\n",
    "\n",
    "for w in emma_words_used_refined:\n",
    "    stem = snowball.stem(w)\n",
    "    emma_stemmed_words2.append(stem)\n",
    "    \n",
    "Counter(emma_stemmed_words2).most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Lemmatization / synset\n",
    "What:  Derive the canonical form ('lemma') of a word\n",
    "    \n",
    "Why:   Can be better than stemming, reduces words to a 'normal' form.\n",
    "    \n",
    "Notes: Uses a dictionary-based approach (slower than stemming)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('dog.n.01'),\n",
       " Synset('frump.n.01'),\n",
       " Synset('dog.n.03'),\n",
       " Synset('cad.n.01'),\n",
       " Synset('frank.n.02'),\n",
       " Synset('pawl.n.01'),\n",
       " Synset('andiron.n.01'),\n",
       " Synset('chase.v.01')]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.corpus.wordnet.synsets('dog')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# What synsets does 'dog' belong to?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Which synset is the one you were thinking of?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# What is its hypernym?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# What about wolves? What synsets does it belong to?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# How closely related are those concepts (dogs and wolves)?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# How closely related are the concepts 'dog' and 'novel'?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Part 3 Part of speech tagging\n",
    "\n",
    "Other:\n",
    "- Analysing data with the Alchemy API\n",
    "- Further Reading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Part of Speech Tagging\n",
    "\n",
    "What:  Determine the part of speech of a word\n",
    "    \n",
    "Why:   This can inform other methods and models such as Named Entity Recognition\n",
    "    \n",
    "Notes: http://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use nltk.pos_tag to parse a sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# (Optional for the enthusiastic)\n",
    "# What verbs did Jane Austen use a lot of?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Part 4\n",
    "###Stopword Removal\n",
    "\n",
    "What:  Remove common words that will likely appear in any text\n",
    "    \n",
    "Why:   They don't tell you much about your text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'c' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-c06df35ae903>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# most of top 25 stemmed tokens are \"worthless\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_common\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'c' is not defined"
     ]
    }
   ],
   "source": [
    "# most of top 25 stemmed tokens are \"worthless\"\n",
    "c.most_common(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# view the list of stopwords\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "sorted(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##################\n",
    "### Exercise  ####\n",
    "##################\n",
    "\n",
    "\n",
    "# Create a variable called stemmed_stops which is the \n",
    "# stemmed version of each stopword in stopwords\n",
    "# Use the stemmer we used up above!\n",
    "\n",
    "# Then create a list called stemmed_tokens_no_stop that \n",
    "# contains only the tokens in stemmed_tokens that aren't in \n",
    "# stemmed_stops\n",
    "\n",
    "# Show the 25 most common stemmed non stop word tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Part 5\n",
    "###Named Entity Recognition\n",
    "\n",
    "What:  Automatically extract the names of people, places, organizations, etc.\n",
    "\n",
    "Why:   Can help you to identify \"important\" words\n",
    "\n",
    "Notes: Training NER classifier requires a lot of annotated training data\n",
    "       Should be trained on data relevant to your task\n",
    "       Stanford NER classifier is the \"gold standard\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentence = 'Ian is an instructor for General Assembly'\n",
    "\n",
    "tokenized = nltk.word_tokenize(sentence)\n",
    "\n",
    "tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tagged = nltk.pos_tag(tokenized)\n",
    "\n",
    "tagged\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "chunks = nltk.ne_chunk(tagged)\n",
    "\n",
    "chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_entities(text):\n",
    "    entities = []\n",
    "    # tokenize into sentences\n",
    "    for sentence in nltk.sent_tokenize(text):\n",
    "        # tokenize sentences into words\n",
    "        # add part-of-speech tags\n",
    "        # use NLTK's NER classifier\n",
    "        chunks = nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(sentence)))\n",
    "        # parse the results\n",
    "        entities.extend([chunk for chunk in chunks if hasattr(chunk, 'label')])\n",
    "    return entities\n",
    "\n",
    "for entity in extract_entities('Ian is an instructor for General Assembly'):\n",
    "    print '[' + entity.label() + '] ' + ' '.join(c[0] for c in entity.leaves())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Part 6\n",
    "###Term Frequency - Inverse Document Frequency (TF-IDF)\n",
    "\n",
    "What:  Computes \"relative frequency\" that a word appears in a document\n",
    "           compared to its frequency across all documents\n",
    "\n",
    "Why:   More useful than \"term frequency\" for identifying \"important\" words in\n",
    "           each document (high frequency in that document, low frequency in\n",
    "           other documents)\n",
    "\n",
    "Notes: Used for search engine scoring, text summarization, document clustering\n",
    "\n",
    "How: \n",
    "    TF(t) = (Number of times term t appears in a document) / (Total number of terms in the document).\n",
    "    IDF(t) = log_e(Total number of documents / Number of documents with term t in it)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample = ['Bob likes sports', 'Bob hates sports', 'Bob likes likes trees']\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vect = CountVectorizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Each row represents a sentence\n",
    "# Each column represents a word\n",
    "vect.fit_transform(sample).toarray()\n",
    "vect.get_feature_names()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer()\n",
    "tfidf.fit_transform(sample).toarray()\n",
    "tfidf.get_feature_names()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the IDF of each word\n",
    "idf = tfidf.idf_\n",
    "print dict(zip(tfidf.get_feature_names(), idf))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###############\n",
    "## Exercise ###\n",
    "###############\n",
    "\n",
    "\n",
    "# for each sentence in sample, find the most \"interesting \n",
    "#words\" by ordering their tfidf in ascending order\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Part 7\n",
    "\n",
    "###LDA - Latent Dirichlet Allocation\n",
    "\n",
    "What:  Way of automatically discovering topics from sentences\n",
    "\n",
    "Why:   Much quicker than manually creating and identifying topic clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import lda\n",
    "\n",
    "# Instantiate a count vectorizer with two additional parameters\n",
    "vect = CountVectorizer(stop_words='english', ngram_range=[1,3]) \n",
    "sentences_train = vect.fit_transform(sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Instantiate an LDA model\n",
    "model = lda.LDA(n_topics=10, n_iter=500)\n",
    "model.fit(sentences_train) # Fit the model \n",
    "n_top_words = 10\n",
    "topic_word = model.topic_word_\n",
    "for i, topic_dist in enumerate(topic_word):\n",
    "    topic_words = np.array(vect.get_feature_names())[np.argsort(topic_dist)][:-n_top_words:-1]\n",
    "    print('Topic {}: {}'.format(i, ', '.join(topic_words)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# EXAMPLE: Automatically summarize a document\n",
    "\n",
    "\n",
    "# corpus of 2000 movie reviews\n",
    "from nltk.corpus import movie_reviews\n",
    "reviews = [movie_reviews.raw(filename) for filename in movie_reviews.fileids()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create document-term matrix\n",
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "dtm = tfidf.fit_transform(reviews)\n",
    "features = tfidf.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# find the most and least \"interesting\" sentences in a randomly selected review\n",
    "def summarize():\n",
    "    \n",
    "    # choose a random movie review    \n",
    "    review_id = np.random.randint(0, len(reviews))\n",
    "    review_text = reviews[review_id]\n",
    "\n",
    "    # we are going to score each sentence in the review for \"interesting-ness\"\n",
    "    sent_scores = []\n",
    "    # tokenize document into sentences\n",
    "    for sentence in nltk.sent_tokenize(review_text):\n",
    "        # exclude short sentences\n",
    "        if len(sentence) > 6:\n",
    "            score = 0\n",
    "            token_count = 0\n",
    "            # tokenize sentence into words\n",
    "            tokens = nltk.word_tokenize(sentence)\n",
    "            # compute sentence \"score\" by summing TFIDF for each word\n",
    "            for token in tokens:\n",
    "                if token in features:\n",
    "                    score += dtm[review_id, features.index(token)]\n",
    "                    token_count += 1\n",
    "            # divide score by number of tokens\n",
    "            sent_scores.append((score / float(token_count + 1), sentence))\n",
    "\n",
    "    # lowest scoring sentences\n",
    "    print '\\nLOWEST:\\n'\n",
    "    for sent_score in sorted(sent_scores)[:3]:\n",
    "        print sent_score[1]\n",
    "\n",
    "    # highest scoring sentences\n",
    "    print '\\nHIGHEST:\\n'\n",
    "    for sent_score in sorted(sent_scores, reverse=True)[:3]:\n",
    "        print sent_score[1]\n",
    "\n",
    "# try it out!\n",
    "summarize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Part 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TextBlob Demo: \"Simplified Text Processing\"\n",
    "# Installation: pip install textblob\n",
    "! pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from textblob import TextBlob, Word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# identify words and noun phrases\n",
    "blob = TextBlob('Greg and Thamali are instructors for General Assembly')\n",
    "blob.words\n",
    "blob.noun_phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sentiment analysis\n",
    "blob = TextBlob('I hate this horrible movie. This movie is not very good.')\n",
    "blob.sentences\n",
    "blob.sentiment.polarity\n",
    "[sent.sentiment.polarity for sent in blob.sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sentiment subjectivity\n",
    "TextBlob(\"I am a cool person\").sentiment.subjectivity # Pretty subjective\n",
    "TextBlob(\"I am a person\").sentiment.subjectivity # Pretty objective\n",
    "# different scores for essentially the same sentence\n",
    "print TextBlob('Greg and Thamali are instructors for General Assembly in Sydney').sentiment.subjectivity\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# singularize and pluralize\n",
    "blob = TextBlob('Put away the dishes.')\n",
    "[word.singularize() for word in blob.words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "[word.pluralize() for word in blob.words]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# spelling correction\n",
    "blob = TextBlob('15 minuets late')\n",
    "blob.correct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# spellcheck\n",
    "Word('parot').spellcheck()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# definitions\n",
    "Word('bank').define()\n",
    "Word('bank').define('v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# translation and language identification\n",
    "blob = TextBlob('Welcome to the classroom.')\n",
    "blob.translate(to='es')\n",
    "blob = TextBlob('Hola amigos')\n",
    "blob.detect_language()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
